{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart を用いた物体検出転移学習\n",
    "Builders Online Series \"Amazon SageMaker JumpStart を用いて機械学習 PoC を IT エンジニアの手で実行する ～ タケノコ好きに向けたキノコ検出問題を添えて ～\"  \n",
    "で登壇した内容を再現するコードです。  \n",
    "前提として、SageMaker Studio で、このノートブックを開き、Kernelは`Data Science`、インスタンスは`ml.t3.medium`を選択します。  \n",
    "\n",
    "## ディレクトリ構成\n",
    "* manifest\n",
    "    * output.manifest  \n",
    "    `train_raw_images/*.jpg`をSageMaker GroundTruth でラベリングした結果。自身でSageMaker GroundTruth でラベリングする際もや、別途用意した画像を使う場合はこのファイルは不要。  \n",
    "    `train_raw_images/*.jpg` をそのまま使い、ラベリングをしたくない場合は使用する\n",
    "* test_crop_images\n",
    "  `test_raw_images/takenoko.jpg`から 512px x 512px の画像をスライドさせながら切り出した画像を格納するフォルダ。  \n",
    "  `8-2. ベルトコンベアを模した推論`で使用する  \n",
    "  初期状態では空ディレクトリ\n",
    "* test_detect_images\n",
    "  `test_crop_images/*.png`にある画像を推論した結果を保存するディレクトリ\n",
    "  `8-2. ベルトコンベアを模した推論`で使用する  \n",
    "  初期状態では空ディレクトリ\n",
    "* test_raw_images\n",
    "  テスト用の画像を配置  \n",
    "  `latitice.png` は格子状にお菓子を配置した画像で、`8-1. 格子状に配置した画像を推論する`で使用する  \n",
    "  `takenoko.jpg` はお菓子を直線に配置した画像で、`8-2. ベルトコンベアを模した推論`で使用する  \n",
    "* train_random_crop_images\n",
    "  学習用にcropした画像を配置する  \n",
    "  `5. ラベルと学習データの整形 ` で使用する\n",
    "  初期状態では空ディレクトリ\n",
    "* train_raw_images\n",
    "  学習用の生画像  \n",
    "\n",
    "## 1. 使用するモジュールのインストールと読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!apt-get update && apt-get upgrade -y\n",
    "!apt-get install libgl1-mesa-dev -y\n",
    "!pip install opencv-python\n",
    "\n",
    "import sagemaker, json, numpy as np, os, boto3, uuid\n",
    "from PIL import Image, ImageDraw, ImageOps, ImageColor, ImageFont\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "import cv2\n",
    "np.random.seed(seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 画像収集\n",
    "* 今回は`./train_raw_images`に格納済\n",
    "* 自身のデータを利用するには画像を差し替える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル有無確認\n",
    "TRAIN_RAWIMAGE_DIR = './train_raw_images/'\n",
    "print(*sorted(glob(TRAIN_RAWIMAGE_DIR+'*.jpg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SageMaker GroundTruth でラベリング\n",
    "### 3-1. ラベリング対象を S3 にアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_PREFIX = 'takenoko_kinoko_gt'\n",
    "GT_JOB_NAME = f'{BASE_PREFIX}-{uuid.uuid4()}'.replace('_','-')\n",
    "bucket = sagemaker.session.Session().default_bucket()\n",
    "!aws s3 rm s3://{bucket}/{BASE_PREFIX} --recursive\n",
    "rawimage_s3_uri = sagemaker.session.Session().upload_data(TRAIN_RAWIMAGE_DIR,key_prefix=BASE_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GroundTruth job name : {GT_JOB_NAME}')\n",
    "print(f'GroundTruth Target : {rawimage_s3_uri}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. ラベリングジョブを作成\n",
    "\n",
    "1. [SageMaker GroundTruth のラベリングジョブ作成画面](https://ap-northeast-1.console.aws.amazon.com/sagemaker/groundtruth?region=ap-northeast-1#/labeling-jobs)にアクセスし、`ラベリングジョブの作成` をクリック\n",
    "2. `ジョブ名`に上のセルの `GroundTruth job name :` 以降の文字列を記入\n",
    "3. `入力データセットの S3 の場所`に上のセルの`GroundTruth Target :`以降の文字列を記入\n",
    "4. `出力データセットの S3 の場所` はデフォルトのまま`入力データセットと同じ場所` を選択\n",
    "5. `データタイプ`は`画像`を選択\n",
    "6. `IAM ロール`は今動かしている SageMaker Studio と同じロールを選択\n",
    "7. `完全なデータセットアップ`をクリック\n",
    "8. `タスクの選択`で、`境界ボックス`を選択\n",
    "9. `次へ`をクリック\n",
    "10. `ワーカータイプ`で`プライベートチーム`を選択\n",
    "11. `プライベートチーム`のプルダウンで作成済のワーカーを選択(要事前に作成)\n",
    "12. 他はデフォルトのまま、`境界ボックスラベリングツール`の部分で、`take`と`kino`というラベルを作成\n",
    "    初期の空白ラベルは１つなので、`take`を入力後、`新しいラベルを追加`をクリックして、`kino` を入力\n",
    "13. `作成`をクリック\n",
    "\n",
    "### 3-3. ラベリング\n",
    "プライベートチームにログインして、タケノコ状のお菓子と、キノコ状のお菓子をそれぞれ矩形で括る\n",
    "\n",
    "##  4.ラベリング結果をダウンロード\n",
    "* ラベリング結果である`output.manifest`をダウンロードする。\n",
    "    * このリポジトリにはあらかじめ`output.manifest`を`./manifest/output.manifest`に用意しているので、ラベリングを割愛したい場合はそちらを使う\n",
    "    * ラベリングをした場合は下のセルのコメントアウトを外す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.session.Session().download_data('./manifest/',key_prefix=f'{BASE_PREFIX}/{GT_JOB_NAME}/manifests/output/output.manifest',bucket=bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ラベルと学習データの整形\n",
    "* SSD MobileNet 1.0 で転移学習するために`./manifest/output.manifest`を加工して、`manifest.json`を作成する\n",
    "* データ形式は、以下を遵守する必要がある\n",
    "```\n",
    "The annotations.json file should should have information for bounding_boxes and their class labels. It should have a dictionary with keys \"images\" and \"annotations\". Value for the \"images\" key should be a list of entries, one for each image of the form {\"file_name\": image_name, \"height\": height, \"width\": width, \"id\": image_id}. Value of the 'annotations' key should be a list of entries, one for each bounding box of the form {\"image_id\": image_id, \"bbox\": [xmin, ymin, xmax, ymax], \"category_id\": bbox_label}.\n",
    "```\n",
    "* 学習データと`manifest.json`はS3に最終的に配置する必要があるが、S3でのデータの配置は以下を遵守する必要がある\n",
    "```\n",
    "input_directory\n",
    "    |--images\n",
    "        |--abc.png\n",
    "        |--def.png\n",
    "    |--annotations.json     \n",
    "```\n",
    "* 学習データは SSD MobileNet 1.0 は 512 x 512 の画像サイズにする必要がある\n",
    "  * 学習用の生データは 1147 x 1108 のため、お菓子が少なくとも 2 個以上写っている場所をランダムに 512 x 512 で切り取る\n",
    "  * 切り取る枚数は書く画像から 5 枚\n",
    "  * ラベリングでくくった矩形の面積の 1/4 以上写っていればお菓子が写った、とカウントする\n",
    "* 512 x 512 に切り取った画像は、ラベリングの情報と当然ずれるので、切り取った座標位置を用いて補正する\n",
    "\n",
    "\n",
    "これらの処理を以下で行い、512 x 512 の画像を`./train_random_crop_images`に、ラベリングデータを`./manifest.json`に出力し、それらを`s3://sagemaker-{region}-{account}/train_randcrop`以下に保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_bbox(l,t,r,b,w,h):\n",
    "    judge = True\n",
    "    \n",
    "    fix_left = 0 if l < 0 else l\n",
    "    fix_top = 0 if t < 0 else t\n",
    "    fix_right = w if r > w else r\n",
    "    fix_bottom = h if b > h else b\n",
    "    \n",
    "    # 領域外なら\n",
    "    if l > w or t > h or r < 0 or b <0:\n",
    "        judge=False\n",
    "    # 基の面積の1/4以下ならアノテーション無しとする\n",
    "    elif (r-l)*(b-t)/4 > (fix_right-fix_left)*(fix_bottom-fix_top):\n",
    "        judge=False\n",
    "    \n",
    "    return judge,(fix_left,fix_top,fix_right,fix_bottom)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('manifest/output.manifest','r') as f:\n",
    "    manifest_line_list = f.readlines()\n",
    "annotation_dict = {\n",
    "    'images':[],\n",
    "    'annotations':[]\n",
    "}\n",
    "IMAGE_SIZE_TUPLE=(512,512)\n",
    "IMAGE_ID = 0\n",
    "\n",
    "OUTPUT_DIR = './train_random_crop_images/'\n",
    "!rm -rf {OUTPUT_DIR}\n",
    "!mkdir {OUTPUT_DIR}\n",
    "\n",
    "for i,manifest_line in enumerate(manifest_line_list):\n",
    "    manifest_dict = json.loads(manifest_line)\n",
    "    filename = manifest_dict['source-ref'].split('/')[-1]\n",
    "    image_size_tuple=(manifest_dict['kinoko-takenoko-aug']['image_size'][0]['width'],manifest_dict['kinoko-takenoko-aug']['image_size'][0]['height'])\n",
    "    raw_img = Image.open(os.path.join(TRAIN_RAWIMAGE_DIR,filename))\n",
    "    for i in range(20):\n",
    "        loop = True\n",
    "        while loop:\n",
    "            rand_x = np.random.randint(0,image_size_tuple[0]-IMAGE_SIZE_TUPLE[0])\n",
    "            rand_y = np.random.randint(0,image_size_tuple[1]-IMAGE_SIZE_TUPLE[1])\n",
    "            crop_img = raw_img.crop((\n",
    "                rand_x,\n",
    "                rand_y,\n",
    "                rand_x + IMAGE_SIZE_TUPLE[0],\n",
    "                rand_y + IMAGE_SIZE_TUPLE[1]\n",
    "            ))\n",
    "            annotation_list = []\n",
    "            for annotation in manifest_dict['kinoko-takenoko-aug']['annotations']:\n",
    "                left = annotation['left'] - rand_x\n",
    "                top = annotation['top'] - rand_y\n",
    "                right = annotation['left'] + annotation['width'] - rand_x\n",
    "                bottom = annotation['top'] + annotation['height'] - rand_y\n",
    "                judge,(left,top,right,bottom) = fix_bbox(left,top,right,bottom,IMAGE_SIZE_TUPLE[0],IMAGE_SIZE_TUPLE[1])\n",
    "                if judge:\n",
    "                    annotation_list.append(\n",
    "                        {\n",
    "                            'bbox':[left,top,right,bottom],\n",
    "                            'category_id':annotation['class_id']\n",
    "                        }\n",
    "                    )\n",
    "            if len(annotation_list) > 1:\n",
    "                loop = False\n",
    "        \n",
    "        save_file_name = f'{str(IMAGE_ID).zfill(5)}_{str(i).zfill(5)}_{filename}'.replace('jpg','png')\n",
    "        crop_img.save(os.path.join(OUTPUT_DIR,save_file_name))\n",
    "        annotation_dict['images'].append(\n",
    "            {\n",
    "                'file_name' : save_file_name,\n",
    "                'height' : IMAGE_SIZE_TUPLE[1],\n",
    "                'width' : IMAGE_SIZE_TUPLE[0],\n",
    "                'id' : IMAGE_ID\n",
    "            }\n",
    "        )\n",
    "        for annotation in annotation_list:                  \n",
    "            annotation_dict['annotations'].append(\n",
    "                {\n",
    "                    'image_id': IMAGE_ID,\n",
    "                    'bbox':annotation['bbox'],\n",
    "                    'category_id':annotation['category_id']\n",
    "                }\n",
    "            )\n",
    "        IMAGE_ID += 1\n",
    "\n",
    "with open('annotations.json','wt') as f:\n",
    "    f.write(json.dumps(annotation_dict))        \n",
    "\n",
    "prefix = OUTPUT_DIR[2:-1]\n",
    "!aws s3 rm s3://{bucket}/{prefix} --recursive\n",
    "        \n",
    "image_s3_uri = sagemaker.session.Session().upload_data(OUTPUT_DIR,key_prefix=f'{prefix}/images')\n",
    "annotatione_s3_uri = sagemaker.session.Session().upload_data('./annotations.json',key_prefix=prefix)\n",
    "paste_str = image_s3_uri.replace('/images','')\n",
    "print(f\"paste string to S3 bucket address：{paste_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 学習する\n",
    "SageMaker JumpStart で転移学習を行う\n",
    "\n",
    "1. SageMaker Studio の左上にある `+` アイコンなどから  Launcher タブを出し、`Get started` の`Explore one-click solutions, models, and tutorials SageMaker JumpStart` をクリックする\n",
    "2. `Search` と書かれた検索窓に `SSD MobileNet` と打ち込むと青い丸に`m`と書かれたアイコンで SSD MobileNet 1.0 と表示されるので、それをクリックする\n",
    "3. やや下にあるFine-tune Model のData Source のラジオボタンを `Enter S3 bucket location` を選択し、`S3 bucket address` に上のセルの `paste string to S3 bucket address: `より後ろの値を貼り付ける\n",
    "4. `Deployment Configuration` をクリックし、`SageMaker Training Instance` で `ML.G4dn.xlarge`を選択する。`Model Name` は任意の名前を入力する（デフォルトのままでも可だが、わからなくなりやすいので、検出したいモノなどを入れるとよい。例：`kinoko-detection-model` など）\n",
    "5. `Hyper-parameters`をクリックし、下記を入力する。（デフォルトのままだと学習が短く急なため変更）\n",
    "    * learning rate : 0.0001\n",
    "    * batch-size : 4\n",
    "    * epochs : 4\n",
    "6. `Train` をクリックして転移学習をスタートさせる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.デプロイする\n",
    "* 任意の名前"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.推論する\n",
    "### 8-1. 格子状に配置した画像を推論する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smr_client = boto3.client('sagemaker-runtime')\n",
    "ENDPOINT_NAME='jumpstart-ftc-kinoko-detection-1'\n",
    "TEST_IMAGE_FILE = 'test_raw_images/lattice.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論対象の画像を開いて変数に格納\n",
    "with open(TEST_IMAGE_FILE, 'rb') as f:\n",
    "    img_bin = f.read()\n",
    "\n",
    "# 推論を実行\n",
    "response = smr_client.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application/x-image', Body=img_bin)\n",
    "\n",
    "# 推論結果を読み込む\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "\n",
    "# 結果を可視化\n",
    "colors = list(ImageColor.colormap.values())\n",
    "image_np = np.array(Image.open(TEST_IMAGE_FILE))\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = plt.axes()\n",
    "ax.imshow(image_np)\n",
    "bboxes, classes, confidences = model_predictions\n",
    "for idx in range(len(bboxes)):\n",
    "    if confidences[idx]>0.6:\n",
    "        left, bot, right, top = bboxes[idx]\n",
    "        x, w = [val * image_np.shape[1] for val in [left, right - left]]\n",
    "        y, h = [val * image_np.shape[0] for val in [bot, top - bot]]\n",
    "        color = colors[hash(classes[idx]) % len(colors)]\n",
    "        class_name = 'take' if classes[idx]=='0' else 'kino'\n",
    "        color = 'blue' if class_name == 'take' else 'red'\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y, \"{} {:.0f}%\".format(class_name, confidences[idx]*100), bbox=dict(facecolor='white', alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-2. ベルトコンベアを模した推論\n",
    "まずは大きい画像から512x512の画像をスライドしながら切り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x ,y = 0,512\n",
    "CROP_SIZE=(512,512)\n",
    "img = Image.open('./test_raw_images/takenoko.jpg')\n",
    "CROP_DIR = './test_crop_images/'\n",
    "!rm -rf {CROP_DIR}\n",
    "!mkdir {CROP_DIR}\n",
    "for i in range(img.size[0]-CROP_SIZE[0]):\n",
    "    crop_img = img.crop((i,y,i+CROP_SIZE[0],y+CROP_SIZE[1]))\n",
    "    file_name = f'{CROP_DIR}{str(i).zfill(5)}.png'\n",
    "    crop_img.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全データ推論して、矩形を描いた画像を生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTSIZE=14\n",
    "LINEWIDTH=4\n",
    "DETECT_DIR='./test_detect_images/'\n",
    "\n",
    "smr_client=boto3.client('sagemaker-runtime')\n",
    "for img_file_path in sorted(glob(f'{CROP_DIR}*.png')):\n",
    "\n",
    "    with open(img_file_path,'rb') as f:\n",
    "        img_bin = f.read()\n",
    "    response = smr_client.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application/x-image', Body=img_bin)\n",
    "    pred=json.loads(response['Body'].read())\n",
    "    bboxes, classes, confidences = pred\n",
    "    img = Image.open(img_file_path)\n",
    "    bboxes, classes, confidences = pred\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for i in range(len(bboxes)):\n",
    "        if confidences[i]>0.7:\n",
    "            left, top, right, bottom = bboxes[i]\n",
    "            left = img.size[0] * left\n",
    "            top = img.size[1] * top\n",
    "            right = img.size[0] * right\n",
    "            bottom = img.size[1] * bottom\n",
    "            text = 'take' if classes[i]=='0' else 'kino'\n",
    "            color = 'blue' if text == 'take' else 'red'\n",
    "            TEXTSIZE=14 if classes[i]=='0' else 18\n",
    "            LINEWIDTH=4 if classes[i]=='0' else 6\n",
    "            draw.rectangle([(left,top),(right,bottom)], outline=color, width=LINEWIDTH)\n",
    "            text += f' {str(round(confidences[i],3))}'\n",
    "            txpos = (left, top-TEXTSIZE-LINEWIDTH//2)\n",
    "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/noto/NotoMono-Regular.ttf\", size=TEXTSIZE)\n",
    "            txw, txh = draw.textsize(text, font=font)\n",
    "            draw.rectangle([txpos, (left+txw, top)], outline=color, fill=color, width=LINEWIDTH)\n",
    "            draw.text(txpos, text, fill='white',font=font)\n",
    "    img.save(img_file_path.replace(CROP_DIR,DETECT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像を動画に書き込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "video = cv2.VideoWriter('./video.mp4',fourcc, 60.0, CROP_SIZE)\n",
    "for img_file_path in sorted(glob(f'{DETECT_DIR}*.png')):\n",
    "    img = cv2.imread(img_file_path)\n",
    "    video.write(img)\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
